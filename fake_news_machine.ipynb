{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPniPUyGli1pi8LgtmW6ASB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BrianSward/colab-work/blob/main/fake_news_machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and such for the learning part\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "8wYMC6FxLPDw"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installs for the web scraping\n",
        "\n",
        "!pip install charset-normalizer\n",
        "!pip install spacy\n",
        "!pip install trafilatura"
      ],
      "metadata": {
        "id": "KmmVsQh2K_cN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for the web scraping\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "from requests.models import MissingSchema\n",
        "import spacy\n",
        "import trafilatura"
      ],
      "metadata": {
        "id": "MnAQf8rYLdgG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/lutzhamel/fake-news/master/data/fake_or_real_news.csv\")"
      ],
      "metadata": {
        "id": "Ad8qIJUf8476"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begin Data Training"
      ],
      "metadata": {
        "id": "fHBsYTCAU5dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['fake'] = data['label'].apply(lambda x: 0 if x == \"REAL\" else 1)\n",
        "data = data.drop(\"label\", axis=1)"
      ],
      "metadata": {
        "id": "GmvKTbMFFjqR"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data[\"text\"], data[\"fake\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
      ],
      "metadata": {
        "id": "1HpotthyFmNK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train) \n",
        "X_test_vectorized = vectorizer.transform(X_test)"
      ],
      "metadata": {
        "id": "uRqeyS9XFpL8"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LinearSVC()\n",
        "clf.fit(X_train_vectorized, y_train)"
      ],
      "metadata": {
        "id": "4qWpHlGQFsdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets test the accuracy of our model\n"
      ],
      "metadata": {
        "id": "A_t7ZKBBU9IN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BSEGfyqyVKfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_score = round(clf.score(X_test_vectorized, y_test), 2)\n",
        "print(f'{_score*100}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0jtVkn7FwfY",
        "outputId": "0d684bbc-60af-47cf-ede1-9f733929955c"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell below will accept a url, and then process it"
      ],
      "metadata": {
        "id": "lLhFpvWuVisa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_url = input(\"Feed me a url!!! \")\n",
        "\n",
        "text_data = {}\n",
        "resp = requests.get(user_url)\n",
        "if resp.status_code == 200:\n",
        "  text_data[user_url] = resp.text\n",
        "\n",
        "def beautifulsoup_extract_text_fallback(response_content):\n",
        "        \n",
        "    # Create the beautifulsoup object:\n",
        "    soup = BeautifulSoup(response_content, 'html.parser')\n",
        "    \n",
        "    # Finding the text:\n",
        "    text = soup.find_all(text=True)\n",
        "    \n",
        "    # Remove unwanted tag elements:\n",
        "    cleaned_text = ''\n",
        "    blacklist = [\n",
        "        '[document]',\n",
        "        'noscript',\n",
        "        'header',\n",
        "        'html',\n",
        "        'meta',\n",
        "        'head', \n",
        "        'input',\n",
        "        'script',\n",
        "        'style',]\n",
        "\n",
        "    # Then we will loop over every item in the extract text and make sure that the beautifulsoup4 tag\n",
        "    # is NOT in the blacklist\n",
        "    for item in text:\n",
        "        if item.parent.name not in blacklist:\n",
        "            cleaned_text += '{} '.format(item)\n",
        "            \n",
        "    # Remove any tab separation and strip the text:\n",
        "    cleaned_text = cleaned_text.replace('\\t', '')\n",
        "    return cleaned_text.strip()\n",
        "    \n",
        "\n",
        "def extract_text_from_single_web_page(url):\n",
        "    \n",
        "    downloaded_url = trafilatura.fetch_url(url)\n",
        "    try:\n",
        "        a = trafilatura.extract(downloaded_url, output_format='json', with_metadata=True, include_comments = False,\n",
        "                            date_extraction_params={'extensive_search': True, 'original_date': True})\n",
        "    except AttributeError:\n",
        "        a = trafilatura.extract(downloaded_url, json_output=True, with_metadata=True,\n",
        "                            date_extraction_params={'extensive_search': True, 'original_date': True})\n",
        "    if a:\n",
        "        json_output = json.loads(a)\n",
        "        return json_output['text']\n",
        "    else:\n",
        "        try:\n",
        "            resp = requests.get(url)\n",
        "            # We will only extract the text from successful requests:\n",
        "            if resp.status_code == 200:\n",
        "                return beautifulsoup_extract_text_fallback(resp.content)\n",
        "            else:\n",
        "                # This line will handle for any failures in both the Trafilature and BeautifulSoup4 functions:\n",
        "                return np.nan\n",
        "        # Handling for any URLs that don't have the correct protocol\n",
        "        except MissingSchema:\n",
        "            return np.nan\n",
        "          \n",
        "single_url = user_url\n",
        "_text = extract_text_from_single_web_page(url=single_url)\n",
        "vectorized_text = vectorizer.transform([_text])\n",
        "\n",
        "if clf.predict(vectorized_text) == [0]:\n",
        "  print(\"with 92% confidence thats real news\")\n",
        "\n",
        "else:\n",
        "  print(\"with 92% confidence thats fake news\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fl165x-UGQs3",
        "outputId": "2c8ba44d-6b97-4a53-d9b4-b218aaabbe77"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feed me a url!!! https://www.rt.com/russia/575170-ukraine-zelensky-decolonization-law/\n",
            "with 92% confidence thats fake news\n"
          ]
        }
      ]
    }
  ]
}